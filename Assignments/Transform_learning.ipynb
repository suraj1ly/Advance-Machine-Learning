{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References used :\n",
    "# https://github.com/quinngroup/dr1dl-pyspark/wiki/Rank-1-Dictionary-Learning-Pseudocode\n",
    "# https://github.com/Deepayan137/K-svd/blob/master/main_vamsi_2.py\n",
    "# https://github.com/Rehan-Ahmad/Dictionary-Learning-Algorithms/blob/master/Dictionary_learning_v2.py\n",
    "\n",
    "# ----------------------------------------Packages Used ------------------------------------------------------#\n",
    "import cv2\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from operator import mul, sub\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from math import floor, sqrt, log10\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.sparse.linalg import lobpcg\n",
    "import seaborn as sns\n",
    "import heapq\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit as omp\n",
    "import timeit\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dict_initiate(train_noisy_patches, dict_size,option):\n",
    "    if option == \"random_point\":\n",
    "        dict_init = np.random.randn(train_noisy_patches.shape[0],dict_size)\n",
    "        # dictionary intialization\n",
    "\n",
    "        # dictionary normalization\n",
    "        \n",
    "    elif option == \"random_data\":\n",
    "        indexes = np.random.random_integers(0, train_noisy_patches.shape[1]-1, dict_size)   # indexes of patches for dictionary elements\n",
    "        dict_init = np.array(train_noisy_patches[:, indexes])  \n",
    "        dict_init  = normalize(dict_init)\n",
    "        pass\n",
    "    elif option ==\"glorot\":\n",
    "        dict_init = np.random.randn(train_noisy_patches.shape[0],dict_size)\n",
    "        glorot_cons = math.sqrt(2/(dict_init.shape[0]+dict_init.shape[1]))\n",
    "        dict_init = dict_init*glorot_cons\n",
    "        pass\n",
    "\n",
    "    print( 'Shape of dictionary : ' , str(dict_init.shape) + '\\n')\n",
    "\n",
    "    return dict_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_update(D, data, matrix_sparse):\n",
    "    dict_new = []\n",
    "    counter = 0 \n",
    "    for i in range(len(D)):\n",
    "        print(\"Counter : \",counter )\n",
    "        counter = counter +1\n",
    "        index_dict = np.where(np.array(D[i]) == 0)\n",
    "        data_temp = np.delete(data,index_dict,axis=1)\n",
    "        data_square = data_temp.T.dot(data_temp)\n",
    "        random_eigen = np.random.randn(D.shape[0],1)\n",
    "        eig_val, eig_vec = lobpcg(data_square,random_eigen,largest = False,maxiter = 100)\n",
    "        dict_new.append(eig_vec)\n",
    "    return dict_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_update(D,dataset_main,sparsity):\n",
    "    sparse_rep = D.T.dot(dataset_main.T)\n",
    "    length_sparse = len(sparse_rep[0])\n",
    "    for i in range(len(sparse_rep)):\n",
    "        index_largest = heapq.nlargest(sparsity, range(len(sparse_rep[i])), sparse_rep[i].take)\n",
    "        not_list = list(set(range(length_sparse)).difference(set(index_largest)))\n",
    "        for j in range(len(not_list)):\n",
    "            sparse_rep[i][j] =0\n",
    "    return sparse_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_learning(dataset_main, dict_size, sparsity,option):\n",
    "    dict_init = dict_initiate(dataset_main.T, dict_size,option)\n",
    "    D = dict_init\n",
    "\n",
    "    matrix_sparse = np.zeros((D.T.dot(dataset_main.T)).shape)         # initializing spare matrix\n",
    "    num_iter = ksvd_iter\n",
    "    print ('\\nTransform Learning .')\n",
    "    print ('-------------------------------')\n",
    "\n",
    "    for k in range(num_iter):\n",
    "        print (\"Stage \" , str(k+1) , \"/\" , str(num_iter) , \"...\")\n",
    "\n",
    "#         matrix_sparse = omp(D, dataset_main, sparsity)\n",
    "        matrix_sparse = sparse_update(D,dataset_main,sparsity)\n",
    "        print(\"Sparse updated !! \")\n",
    "        D = dict_update(D, dataset_main, matrix_sparse)\n",
    "        \n",
    "        print ('\\r- Dictionary updating complete.\\n')\n",
    "        print(\"Reconstruction Error in  \",(k+1),\" Iteration : \",recons_error(dataset_main,D,matrix_sparse))\n",
    "\n",
    "    return D, matrix_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_image(file_path):\n",
    "    '''Checking file to be image with extension .pgm\n",
    "    Arguments : \n",
    "    file : filename to be check for image or not\n",
    "    Return true if file is image and false if not image.\n",
    "    \n",
    "    '''\n",
    "    if cv2.imread(file_path) is None or cv2.imread(file_path).size == 0: \n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_read_yale_1(directory_loc,size):\n",
    "    '''Create dataset for particular directory \n",
    "    Parameters :\n",
    "    directory_loc : location where all classes data are present\n",
    "    \n",
    "    '''\n",
    "    list_file  = os.listdir(directory_loc)\n",
    "    dataset = [[] for i in list_file]\n",
    "    label = [[] for i in list_file]\n",
    "    for i in range(len(list_file)):\n",
    "        print(\"Counter for directory: \",i)\n",
    "        images  = os.listdir(directory_loc + \"/\"+list_file[i])\n",
    "        for j in range(len(images)):\n",
    "#             if j%250==0:\n",
    "#                 print(\"Counter : \",j)\n",
    "            if check_file_image(str(directory_loc+\"/\"+list_file[i]+\"/\"+images[j])):\n",
    "                img = cv2.imread(str(directory_loc+\"/\"+list_file[i]+\"/\"+images[j]),0)\n",
    "                img = cv2.resize(img,(size,size))\n",
    "                img = img.ravel()\n",
    "                dataset[i].append(img)\n",
    "                label[i].append(i)\n",
    "                \n",
    "    return dataset,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data,label,train_test_split):\n",
    "    '''Make random shuffle of data and split into train and test set\n",
    "    Parameters are as:\n",
    "    data: whole dataset to split\n",
    "    label : whole labels corresponding to each data points\n",
    "    train_test_split : ration of split i.e, for ex: 10/7, if want to split 7:3\n",
    "    '''\n",
    "    split = int(len(data)/train_test_split)\n",
    "    data_sample=[]\n",
    "    for i in range(len(data)):\n",
    "        temp=[]\n",
    "        temp.append(data[i])\n",
    "        temp.append(label[i])\n",
    "        data_sample.append(temp)\n",
    "    data_sample = random.sample(data_sample,len(data_sample))\n",
    "    data=[]\n",
    "    label=[]\n",
    "    for i in range(len(data_sample)):\n",
    "        data.append(data_sample[i][0])\n",
    "        label.append(data_sample[i][1])\n",
    "    return np.array(data[:split]),np.array(label[:split]),np.array(data[split:]),np.array(label[split:]),np.array(data),np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_drive(data,label):\n",
    "    '''Fuction to return the model fitted on data and label given \n",
    "    Parameters : \n",
    "    data : train data used for fitting the model \n",
    "    label : suprvised label for fitting the model \n",
    "    '''\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(data,label)\n",
    "    return gnb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_dict_model(gnb,D,test_data,train_data,sparsity):\n",
    "    \n",
    "    omp_obj = omp(n_nonzero_coefs=sparsity)\n",
    "    omp_test  = omp_obj.fit(D, test_data.T)\n",
    "    test_data = copy.deepcopy(omp_test.coef_)\n",
    "    return test_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recons_error(X,D,Z):\n",
    "    error  = X - (D.dot(Z)).T\n",
    "    return np.linalg.norm(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(dataset_main,label_main,title,classes):\n",
    "    #TSNE Plot for glass dataset\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(dataset_main)\n",
    "\n",
    "    df_subset = pd.DataFrame()\n",
    "    df_subset['X'] = tsne_results[:,0]\n",
    "    df_subset['y']=label_main\n",
    "    df_subset['Y'] = tsne_results[:,1]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.title(title)\n",
    "    sns.scatterplot(\n",
    "        x=\"X\", y=\"Y\",\n",
    "        hue=\"y\",\n",
    "        palette=sns.color_palette(\"hls\", classes),\n",
    "        data=df_subset,\n",
    "        legend=\"full\",\n",
    "        alpha=1.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict_label,true_label):\n",
    "    count = 0\n",
    "    for i in range(len(predict_label)):\n",
    "        if int(predict_label[i]) == int(true_label[i]):\n",
    "            count = count +1\n",
    "    return count/len(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"dataset_main.npy\",dataset_main)\n",
    "np.save(\"label_main.npy\",label_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter for directory:  0\n",
      "Counter for directory:  1\n",
      "Counter for directory:  2\n",
      "Counter for directory:  3\n",
      "Counter for directory:  4\n"
     ]
    }
   ],
   "source": [
    "dataset,label = data_read_yale_1(\"./Dataset/Question_3\",20)\n",
    "dataset_main = []\n",
    "for i in dataset:\n",
    "    dataset_main = dataset_main + i\n",
    "label_main = []\n",
    "for i in label:\n",
    "    label_main = label_main + i\n",
    "label_main = np.array(label_main)\n",
    "dataset_main = np.array(dataset_main)\n",
    "# Dictionary : 1024 * 1000\n",
    "# Sparse matrix : 1000 * no of datapoints\n",
    "train_data,train_label,test_data,test_label,data,label = data_split(dataset_main,label_main,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(train_data,train_label,\"TSNE Plot for Yale Face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dict=1000   \n",
    "ksvd_iter = 10   \n",
    "max_sparsity = 1\n",
    "D,mat_sparse = transform_learning(train_data,num_dict,max_sparsity,\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing Accuracy using Model Methodology\n",
    "gnb = model_drive(mat_sparse,train_label)\n",
    "accuracy = gnb.score(mat_sparse,train_label)\n",
    "print(\"Training Accuracy : \", accuracy)\n",
    "test_data_sparse = predict_test_dict_model(gnb,D,test_data,train_data,max_sparsity)\n",
    "print(\"Testing Accuracy : \", gnb.score(test_data_sparse,test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
